from pyspark.sql import SparkSession
from pyspark.sql.functions import col, sum
     

# Step 1: Initialize Spark Session
spark = SparkSession.builder \
    .appName("Product Data Analysis") \
    .getOrCreate()

# Step 2: Create DataFrame with product data
data = [
    (1, "Laptop", "Electronics", 800, 10, 4.5),
    (2, "Smartphone", "Electronics", 500, 20, 4.3),
    (3, "Desk Chair", "Furniture", 150, 15, 4.2),
    (4, "Monitor", "Electronics", 200, 12, 4.1),
    (5, "Table", "Furniture", 300, 8, 4.0),
    (6, "Washing Machine", "Appliances", 700, 5, 4.6),
    (7, "Refrigerator", "Appliances", 1200, 3, 4.8),
    (8, "Headphones", "Electronics", 100, 25, 4.4),
    (9, "Microwave", "Appliances", 150, 7, 4.3),
    (10, "Sofa", "Furniture", 600, 4, 4.7)
]

# Define column names
columns = ["ProductID", "ProductName", "Category", "Price", "StockQuantity", "Rating"]

# Create DataFrame
df = spark.createDataFrame(data, columns)

# Show the initial DataFrame
print("Original DataFrame:")
df.show()

# Step 3: Sort by Price in descending order and then by Category in ascending order
sorted_df = df.orderBy(col("Price").desc(), col("Category").asc())
print("\nSorted DataFrame (by Price desc, Category asc):")
sorted_df.show()

sales_by_category = df.withColumn("TotalSales", col("Price") * col("StockQuantity")) \
    .groupBy("Category") \
    .agg(sum("TotalSales").alias("TotalSalesAmount"))

print("\nTotal Sales Amount by Category:")
sales_by_category.show()

sales_by_product = df.withColumn("TotalSales", col("Price") * col("StockQuantity")) \
    .groupBy("ProductName") \
    .agg(
        sum("TotalSales").alias("TotalSalesAmount"),
        sum("StockQuantity").alias("TotalQuantitySold")
    )

print("\nTotal Sales Amount and Total Quantity Sold for Each Product:")
sales_by_product.show()

# Stop Spark Session
spark.stop()
